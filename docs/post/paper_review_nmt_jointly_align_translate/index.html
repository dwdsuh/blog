<!DOCTYPE html>
<html lang="en-us">
	<head>
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="author" content="Day1">
<meta name="description" content="Describe your website">
<meta name="generator" content="Hugo 0.55.6" />
<title>[Paper Review] NMT by Jointly Learning to Align and Translate</title>
<link rel="shortcut icon" href="https://dwdsuh.github.io/blog/images/favicon.ico">
<link rel="stylesheet" href="https://dwdsuh.github.io/blog/css/style.css">
<link rel="stylesheet" href="https://dwdsuh.github.io/blog/css/highlight.css">



<link rel="stylesheet" href="https://dwdsuh.github.io/blog/css/monosocialiconsfont.css">



<link href="https://dwdsuh.github.io/blog/index.xml" rel="alternate" type="application/rss+xml" title="Dayone&#39;s Blog" />


<meta property="og:title" content="[Paper Review] NMT by Jointly Learning to Align and Translate" />
<meta property="og:description" content="Neural Machine Translation by Jointly Learning to Align and Translate Paper URL: Click Here
Outline  Motivation: A Fixed-length Vector is a Hurdle  1.1. Basic Encoder-Decoder Architecture
1.2. The Structural Problem with the Basic Architecture
 Contribution: the Advent of Alignment Model (aka Attention)  2.1. Model Architecture
 ​2.1.1. [Encoder: Bidirectional RNN](https://github.com/dwdsuh/Paper_Review/blob/master/NMTbyJointlyAlignTranslate.md#211-encoder-bidirectional-rnn) ​2.1.2. [Decoder: Model Joint Learns How to Align and Translate](https://github.com/dwdsuh/Paper_Review/blob/master/NMTbyJointlyAlignTranslate.md#212-decoder-model-jointly-learns-how-to--align-and-translate)  2.2. Training Procedure
2.3. Result" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://dwdsuh.github.io/blog/post/paper_review_nmt_jointly_align_translate/" />
<meta property="article:published_time" content="2019-07-31T15:06:35&#43;09:00"/>
<meta property="article:modified_time" content="2019-07-31T15:06:35&#43;09:00"/>



<meta itemprop="name" content="[Paper Review] NMT by Jointly Learning to Align and Translate">
<meta itemprop="description" content="Neural Machine Translation by Jointly Learning to Align and Translate Paper URL: Click Here
Outline  Motivation: A Fixed-length Vector is a Hurdle  1.1. Basic Encoder-Decoder Architecture
1.2. The Structural Problem with the Basic Architecture
 Contribution: the Advent of Alignment Model (aka Attention)  2.1. Model Architecture
 ​2.1.1. [Encoder: Bidirectional RNN](https://github.com/dwdsuh/Paper_Review/blob/master/NMTbyJointlyAlignTranslate.md#211-encoder-bidirectional-rnn) ​2.1.2. [Decoder: Model Joint Learns How to Align and Translate](https://github.com/dwdsuh/Paper_Review/blob/master/NMTbyJointlyAlignTranslate.md#212-decoder-model-jointly-learns-how-to--align-and-translate)  2.2. Training Procedure
2.3. Result">


<meta itemprop="datePublished" content="2019-07-31T15:06:35&#43;09:00" />
<meta itemprop="dateModified" content="2019-07-31T15:06:35&#43;09:00" />
<meta itemprop="wordCount" content="464">



<meta itemprop="keywords" content="" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="[Paper Review] NMT by Jointly Learning to Align and Translate"/>
<meta name="twitter:description" content="Neural Machine Translation by Jointly Learning to Align and Translate Paper URL: Click Here
Outline  Motivation: A Fixed-length Vector is a Hurdle  1.1. Basic Encoder-Decoder Architecture
1.2. The Structural Problem with the Basic Architecture
 Contribution: the Advent of Alignment Model (aka Attention)  2.1. Model Architecture
 ​2.1.1. [Encoder: Bidirectional RNN](https://github.com/dwdsuh/Paper_Review/blob/master/NMTbyJointlyAlignTranslate.md#211-encoder-bidirectional-rnn) ​2.1.2. [Decoder: Model Joint Learns How to Align and Translate](https://github.com/dwdsuh/Paper_Review/blob/master/NMTbyJointlyAlignTranslate.md#212-decoder-model-jointly-learns-how-to--align-and-translate)  2.2. Training Procedure
2.3. Result"/>
<meta name="twitter:site" content="@https://www.twitter.com/"/>


    </head>
<body>
    <nav class="main-nav">
	
		<a href='https://dwdsuh.github.io/blog/'> <span class="arrow">←</span>Home</a>
	

	
 		<a href='/blog/about/'>About</a>
  	

	
		<a class="cta" href="https://dwdsuh.github.io/blog/index.xml">Subscribe</a>
	
</nav>

    <section id="wrapper">
        
        
<article class="post">
    <header>
        <h1>[Paper Review] NMT by Jointly Learning to Align and Translate</h1>
        <h2 class="subtitle"></h2>
        <h2 class="headline">
        July 31, 2019
        <br>
        
        </h2>
    </header>
    <section id="post-body">
        

<h1 id="neural-machine-translation-by-jointly-learning-to-align-and-translate">Neural Machine Translation by Jointly Learning to Align and Translate</h1>

<hr />

<p>Paper URL: <a href="https://arxiv.org/pdf/1409.0473.pdf">Click Here</a></p>

<h2 id="outline">Outline</h2>

<ol>
<li><a href="https://github.com/dwdsuh/Paper_Review/blob/master/NMTbyJointlyAlignTranslate.md#1-motivation-a-fixed-length-vector-is-a-hurdle">Motivation: A Fixed-length Vector is a Hurdle</a></li>
</ol>

<p>1.1. <a href="https://github.com/dwdsuh/Paper_Review/blob/master/NMTbyJointlyAlignTranslate.md#1-1-a-basic-encoder-decoder-architecture">Basic Encoder-Decoder Architecture</a></p>

<p>1.2. <a href="https://github.com/dwdsuh/Paper_Review/blob/master/NMTbyJointlyAlignTranslate.md#12-the-structural-problem-with-the-basic-architecture">The Structural Problem with the Basic Architecture</a></p>

<ol>
<li><a href="https://github.com/dwdsuh/Paper_Review/blob/master/NMTbyJointlyAlignTranslate.md#2-contribution-the-advent-of-alignment-model-aka-attention">Contribution: the Advent of Alignment Model (aka Attention)</a></li>
</ol>

<p>2.1. <a href="https://github.com/dwdsuh/Paper_Review/blob/master/NMTbyJointlyAlignTranslate.md#21-model-architecture">Model Architecture</a></p>

<pre><code> ​2.1.1. [Encoder: Bidirectional RNN](https://github.com/dwdsuh/Paper_Review/blob/master/NMTbyJointlyAlignTranslate.md#211-encoder-bidirectional-rnn)

 ​2.1.2. [Decoder: Model Joint Learns How to Align and Translate](https://github.com/dwdsuh/Paper_Review/blob/master/NMTbyJointlyAlignTranslate.md#212-decoder-model-jointly-learns-how-to--align-and-translate)
</code></pre>

<p>2.2. <a href="https://github.com/dwdsuh/Paper_Review/blob/master/NMTbyJointlyAlignTranslate.md#22-training-procedure">Training Procedure</a></p>

<p>2.3. <a href="https://github.com/dwdsuh/Paper_Review/blob/master/NMTbyJointlyAlignTranslate.md#23-result">Result</a></p>

<hr />

<h2 id="1-motivation-a-fixed-length-vector-is-a-hurdle">1. Motivation: A Fixed-length Vector is a Hurdle</h2>

<blockquote>
<p>&ldquo;the use of a fixed-length vector is a bottleneck in improving the perfomance of [the] basic encoder-decoder architecture&rdquo; (1)</p>

<p>&ldquo;Cho et al.(2014b) showed that indeed the performance of a biasic encoder-decoder deteriorates rapidly as the length of an input sentence increases&rdquo;(1)</p>
</blockquote>

<h3 id="1-1-basic-encoder-decoder-architecture">1. 1. Basic Encoder-Decoder Architecture:</h3>

<p><img src="https://github.com/dwdsuh/blog/blob/master/content/post/resources/_gen/images/encoder_decoder-architecture.png" alt="Figure 1" /></p>

<ul>
<li>Encoder</li>
</ul>

<p><img src="https://github.com/dwdsuh/blog/blob/master/content/post/resources/_gen/images/basicEncoder_pic.png" alt="Basic Encoder" /></p>

<ul>
<li>Input: a sequece of vectors: <strong>x</strong>=(x[1], x[1], x[3] &hellip; x[T])</li>
<li>hidden state: h[t]=f (x[t] , h[t-1])</li>

<li><p>A fixed-length vector: c=q({h[1], h[2], &hellip; h[T]})  (which is <strong>z</strong> in the <em>Figure 1</em>)</p></li>

<li><p>Decoder</p></li>
</ul>

<p><img src="https://github.com/dwdsuh/blog/blob/master/content/post/resources/_gen/images/basicDecoder_pic.png" alt="Basic Decoder" /></p>

<ul>
<li><p>hidden state: s[t] = g(s[t-1], y[t-1], c)</p></li>

<li><p>output: <strong>y</strong></p>

<p><img src="https://github.com/dwdsuh/blog/blob/master/content/post/resources/_gen/images/basicDecoder.png" alt="Conditional Probability" /></p></li>
</ul>

<h3 id="1-2-the-structural-problem-with-the-basic-architecture">1.2. The Structural Problem with the Basic Architecture</h3>

<ul>
<li>The basic architecture should compress all information in a input sentence into a fixed-length vector, regardless of the sentence length.</li>
<li>The inflexibility of a fixed-length vector has a harmful effect on the performance especially when the input sentence gets longer.</li>
</ul>

<p><img src="https://github.com/dwdsuh/blog/blob/master/content/post/resources/_gen/images/longersentecesucks.png" alt="Figure2" /></p>

<p>​ * if you are not familiar with the basic encoder-decoder architecture check out <a href="https://arxiv.org/pdf/1406.1078.pdf">Cho <em>et al</em> (2014a)</a> and  <a href="https://www.aclweb.org/anthology/W14-4012">Cho <em>et al</em> (2014b)</a></p>

<h2 id="2-contribution-the-advent-of-alignment-model-aka-attention">2. Contribution: the Advent of Alignment model (aka Attention)</h2>

<blockquote>
<p>&ldquo;[We] propose to extend [a fixed-length vector] by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly&rdquo;(1)</p>
</blockquote>

<h3 id="2-1-model-architecture">2.1. Model Architecture</h3>

<p><img src="https://github.com/dwdsuh/blog/blob/master/content/post/resources/_gen/images/jointlymodel.png" alt="jointlymodel" /></p>

<h4 id="2-1-1-encoder-bidirectional-rnn">2.1.1. Encoder: Bidirectional RNN</h4>

<ul>
<li>Why BiRNN?</li>
</ul>

<p>Allow h[j] to summerize both the preceding words and the following words, focusing on the neighborhood of x[j].</p>

<ul>
<li>Hidden State <strong>h</strong>: Concatenating <em>forward hidden state</em> and <em>backward hidden state</em></li>
</ul>

<p><img src="https://github.com/dwdsuh/blog/blob/master/content/post/resources/_gen/images/BiRNNhiddenstate.png" alt="BiRNN hidden state" /></p>

<h4 id="2-1-2-decoder-model-jointly-learns-how-to-align-and-translate">2.1.2. Decoder: Model Jointly Learns How to  Align and Translate</h4>

<ul>
<li>Overview</li>
</ul>

<p><img src="https://github.com/dwdsuh/blog/blob/master/content/post/resources/_gen/images/jointlyoverview.png" alt="overview" /></p>

<ul>
<li><p>Model in detail: Activation functions</p>

<ol>
<li><strong>g</strong> : deep output with a single maxout hidden layer</li>
</ol>

<p><img src="https://github.com/dwdsuh/blog/blob/master/content/post/resources/_gen/images/activationFunctionG.png" alt="activationfunctionG" /></p>

<ol>
<li><strong>f</strong> : GRU

<br /></li>
</ol>

<p><img src="https://github.com/dwdsuh/blog/blob/master/content/post/resources/_gen/images/activationfunctionF.png" alt="activationfunctionG" /></p>

<ul>
<li>z[i] is a update gate that decides the ratio between the previous hidden state and the new hidden state.</li>
<li>for the more detailed explanation of GRU, check out  <a href="https://arxiv.org/pdf/1406.1078.pdf">Cho <em>et al</em> (2014a)</a>

<br /></li>
</ul>

<ol>
<li><strong>a</strong> : single-layer perceptron

<br /></li>
</ol>

<p><img src="https://github.com/dwdsuh/blog/blob/master/content/post/resources/_gen/images/activationfunctionA.png" alt="activationfunctionA" /></p></li>
</ul>

<h3 id="2-2-training-procedure">2.2. Training Procedure</h3>

<ul>
<li><p>Dataset</p>

<ul>
<li>The bilingual, parallel corpora provided by ACL WMT &lsquo;14 (English-French parallel corpora)</li>
<li>no monolingual corpus</li>
</ul></li>

<li><p>Preprocessing</p>

<ul>
<li>simple toknenization(Moses)</li>
<li>utilized 30,000 most frequently used words</li>
</ul></li>

<li><p>Parameter Initialization</p></li>

<li><p>Training</p>

<ul>
<li>optimizer: SGD</li>
<li>Adapting the learning rate: <a href="https://arxiv.org/pdf/1212.5701.pdf">Adadelta</a> (ε = 10^(−6) and ρ = 0.95)</li>
<li>batch_size:  80 sentences</li>
<li>normalized the L2-norm of the gradient of the cost function each time to be at most a predefined threshold of 1, when the norm was larger than the threshold. <a href="http://proceedings.mlr.press/v28/pascanu13.pdf">Pascanu <em>et al</em> (2013b)</a>

<br /></li>
</ul></li>
</ul>

<h3 id="2-3-result">2.3. Result</h3>

<p><img src="https://github.com/dwdsuh/blog/blob/master/content/post/resources/_gen/images/jointlyresult.png" alt="result" /></p>

    </section>
</article>

<footer id="post-meta" class="clearfix">
    <a href="https://twitter.com/Your%20Twitter%20account">
    <img class="avatar" src="https://dwdsuh.github.io/blog/images/avatar.png">
    <div>
        <span class="dark">Day1</span>
        <span>I&#39;m a Bayesian/Data Scientist/AI Engineer.</span>
    </div>
    </a>
    <section id="sharing">
        <a class="twitter" href="https://twitter.com/intent/tweet?text=https%3a%2f%2fdwdsuh.github.io%2fblog%2fpost%2fpaper_review_nmt_jointly_align_translate%2f - %5bPaper%20Review%5d%20NMT%20by%20Jointly%20Learning%20to%20Align%20and%20Translate by @Your%20Twitter%20account"><span class="icon-twitter"> tweet</span></a>

<a class="facebook" href="#" onclick="
    window.open(
      'https://www.facebook.com/sharer/sharer.php?u='+encodeURIComponent(location.href),
      'facebook-share-dialog',
      'width=626,height=436');
    return false;"><span class="icon-facebook-rect"> Share</span>
</a>

    </section>
</footer>

<div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "spf13" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>

<ul id="post-list" class="archive readmore">
    <h3>Read more</h3>

    
    
    
        <li>
            <a href="/blog/post/shell_script_programming_2/">[Bash] Shell Script Programming<aside class="dates">Jul 28 2019</aside></a>
        </li>
    
        <li>
            <a href="/blog/post/git_tutorial/">[Git] Git Command Cheat Sheet / Git 명령어 정리<aside class="dates">Jul 27 2019</aside></a>
        </li>
    
        <li>
            <a href="/blog/post/searhing_stragegy/">[CS] Searhing Strategy<aside class="dates">Jul 23 2019</aside></a>
        </li>
    
        <li>
            <a href="/blog/post/estimator/">[Python] tf.estimator / 텐서플로우 에스티메이터 활용하기<aside class="dates">Jul 23 2019</aside></a>
        </li>
    
        <li>
            <a href="/blog/post/linux_command_cheat_sheet/">[Bash] Linux Command Cheat Sheet / 리눅스 명령어 정리<aside class="dates">Jul 22 2019</aside></a>
        </li>
    
        <li>
            <a href="/blog/post/preprocessing/">[Python] Preprocessing &#34;Big Data&#34; / 대용량 파일 전처리<aside class="dates">Jul 14 2019</aside></a>
        </li>
    
</ul>



        <footer id="footer">
    
        <div id="social">

	
	
    
    <a class="symbol" href="https://www.dribbble.com/">
        circledribble
    </a>
    
    <a class="symbol" href="https://www.facebook.com/">
        circlefacebook
    </a>
    
    <a class="symbol" href="https://www.github.com/">
        circlegithub
    </a>
    
    <a class="symbol" href="https://www.twitter.com/">
        circletwitterbird
    </a>
    


</div>

    
    <p class="small">
    
        © Copyright 2019 Day1
    
    </p>
</footer>

    </section>
    <script src="//ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>
<script src="https://dwdsuh.github.io/blog/js/main.js"></script>
<script src="https://dwdsuh.github.io/blog/js/highlight.js"></script>
<script>hljs.initHighlightingOnLoad();</script>





</body>
</html>
